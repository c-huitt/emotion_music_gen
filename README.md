# AUDIO-TEXT EMOTION CLASSIFICATION IN SPEECH FOR ADAPTIVE MUSIC SYNTHESIS

## Overview
This study focuses on developing a multimodal emotion classification system that detects emotions from audio and generates personalized music based on the identified emotional context. The system processes and augments audio data from standardized datasets (RAVDESS, CREMA, SAVEE, and TESS) to ensure robustness. Audio features such as Mel-Frequency Cepstral Coefficients (MFCCs) and spectral characteristics are extracted, while text transcriptions, generated using OpenAIâ€™s Whisper model, are encoded with Word2Vec embeddings. Emotion classification is performed using a 7-layer Convolutional Neural Network (CNN) and a hybrid CNN-LSTM model. The integration of audio and text features improves the accuracy of emotion detection, which is then linked to dynamic music generation. This project offers a new framework for creating emotionally resonant content, with potential applications in media production, storytelling, and creative industries.

## Features

### Sentiment Analysis
- **Audio-Based Emotion Recognition**: Extracts key audio features such as MFCCs and spectral characteristics to detect emotions from speech.
- **Text-Based Emotion Recognition**: Utilizes text transcriptions generated by OpenAI's Whisper model and encodes them using Word2Vec embeddings for emotion detection.
- **Multimodal Integration**: Combines both audio and text-based emotion features to improve classification performance.
- **Emotion Classification**: Classifies emotions into categories like happiness, sadness, anger, surprise, etc.

### Music Generation
- **Emotion-Driven Music Synthesis**: Selects personalized music based on the detected emotional context.

## Document Overview

- **/datasets**: contains raw and combined datasets.
- **/feature_csv**: extracted features from audio & text. 
- **/models**: contains emotion classification models.
- **/plots**: configured visualization of our model's results.
- **/predicted_emotions**: contains CSVs for actual emotions & predicted emotions.
- **/transcriptions**: audio transcription using WhisperAI.
- **/transformer_csv**: contains extracted features from the Wav2Vec models for each dataset in CSVs.
- **cnn7layer.py**: CNN 7-layered model with 94.25% accuracy. (Best model)
- **cnn_7layer_8layer_l2_lstm.py**: Experimented with 7-layered, 8-layered with L2 and LSTM models for improving accuracy.
- **cnn_feature.comb.py**: combining text and audio features.
- **data_preprocess_and_text_feature_extraction_svm.py**: text feature extraction. Tried simple SVM and best fit SVM for increasing accuracy.
- **music_gen.py**: converts the classified emotion to music.
- **svm.py**: Experimented SVM for emotion classification.
- **svm_savee.py**: SVM using Savee(smallest dataset) to check for accuracy.
- **wav2vec.py**: feature extractions using Wav2Vec model.
